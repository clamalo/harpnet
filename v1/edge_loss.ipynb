{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "os.system('ulimit -n 1024')\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "cwd_dir = (os.path.abspath(os.path.join(os.getcwd())))\n",
    "sys.path.insert(0, cwd_dir)\n",
    "from utils.model import UNetWithAttention\n",
    "from utils.utils3 import *\n",
    "import utils.constants as constants\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_month = (1979, 10)\n",
    "last_month = (1980, 9)\n",
    "train_test = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain1 = 28\n",
    "domain2 = 29\n",
    "\n",
    "train1_dataloader, test1_dataloader = generate_dataloaders(domain1, first_month, last_month, train_test)\n",
    "train2_dataloader, test2_dataloader = generate_dataloaders(domain2, first_month, last_month, train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "for i in range(3):\n",
    "    # Load the first sample from the first domain\n",
    "    input1, target1, times1 = next(iter(train1_dataloader))\n",
    "    input1, target1, times1 = input1[0], target1[0], times1[0]\n",
    "    # convert times1 from timestamp to numpy datetime64\n",
    "    times1 = np.datetime64(datetime.utcfromtimestamp(times1.item()))\n",
    "\n",
    "    input2, target2, times2 = next(iter(train2_dataloader))\n",
    "    input2, target2, times2 = input2[0], target2[0], times2[0]\n",
    "    # convert times2 from timestamp to numpy datetime64\n",
    "    times2 = np.datetime64(datetime.utcfromtimestamp(times2.item()))\n",
    "\n",
    "    edge1 = target1[:, -1]\n",
    "    edge2 = target2[:, 0]\n",
    "\n",
    "    # find mse between edge1 and edge2\n",
    "    mse = nn.MSELoss()\n",
    "    loss = mse(edge1, edge2)\n",
    "\n",
    "    input = np.concatenate([input1, input2], axis=1)\n",
    "    target = np.concatenate([target1, target2], axis=1)\n",
    "\n",
    "    # plot just the input (no subplot)\n",
    "    plt.imshow(target, vmin=0, vmax=1)\n",
    "    # add a line at the edg1 and edge2\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(times1)\n",
    "    print(times2)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_to_two_relationship = 'left'\n",
    "\n",
    "model1 = UNetWithAttention(1, 1, output_shape=(64,64)).to(constants.device).eval()\n",
    "model2 = UNetWithAttention(1, 1, output_shape=(64,64)).to(constants.device).eval()\n",
    "\n",
    "checkpoint1 = torch.load(f'{constants.checkpoints_dir}{domain1}/1_model.pt')\n",
    "model1.load_state_dict(checkpoint1['model_state_dict'])\n",
    "\n",
    "checkpoint2 = torch.load(f'{constants.checkpoints_dir}{domain2}/1_model.pt')\n",
    "model2.load_state_dict(checkpoint2['model_state_dict'])\n",
    "\n",
    "print('weights loaded')\n",
    "\n",
    "# test1_dataloader\n",
    "# test2_dataloader\n",
    "\n",
    "# for i, (inputs, targets, times) in tqdm(enumerate(test1_dataloader), total=len(test1_dataloader)):\n",
    "\n",
    "target_edge_losses = []\n",
    "output_edge_losses = []\n",
    "\n",
    "for i, ((inputs1, targets1, times1), (inputs2, targets2, times2)) in tqdm(enumerate(zip(test1_dataloader, test2_dataloader)), total=len(test1_dataloader)):\n",
    "\n",
    "    inputs1, targets1 = inputs1.to(constants.device), targets1.to(constants.device)\n",
    "    inputs2, targets2 = inputs2.to(constants.device), targets2.to(constants.device)\n",
    "\n",
    "    outputs1 = model1(inputs1)\n",
    "    outputs2 = model2(inputs2)\n",
    "\n",
    "    if one_to_two_relationship == 'left':\n",
    "        domain1_target_edge = targets1[:, :, -1]\n",
    "        domain2_target_edge = targets2[:, :, 0]\n",
    "        domain1_output_edge = outputs1[:, :, -1]\n",
    "        domain2_output_edge = outputs2[:, :, 0]\n",
    "\n",
    "    edge_loss = nn.MSELoss()\n",
    "    target_edge_loss = edge_loss(domain1_target_edge, domain2_target_edge).item()\n",
    "    output_edge_loss = edge_loss(domain1_output_edge, domain2_output_edge).item()\n",
    "\n",
    "    target_edge_losses.append(target_edge_loss)\n",
    "    output_edge_losses.append(output_edge_loss)\n",
    "\n",
    "print(np.mean(target_edge_losses))\n",
    "print(np.mean(output_edge_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "os.system('ulimit -n 1024')\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "cwd_dir = (os.path.abspath(os.path.join(os.getcwd())))\n",
    "sys.path.insert(0, cwd_dir)\n",
    "from utils.model import UNetWithAttention\n",
    "from utils.utils3 import *\n",
    "import utils.constants as constants\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_dataloaders(domain, first_month, last_month, train_test):\n",
    "    input_file_paths = []\n",
    "    target_file_paths = []\n",
    "    times_file_paths = []\n",
    "    first_month = datetime(first_month[0], first_month[1], 1)\n",
    "    last_month = datetime(last_month[0], last_month[1], 1)\n",
    "    current_month = first_month\n",
    "    while current_month <= last_month:\n",
    "        input_fp = f'{constants.domains_dir}{domain}/input_{current_month.year}_{current_month.month:02d}.npy'\n",
    "        target_fp = f'{constants.domains_dir}{domain}/target_{current_month.year}_{current_month.month:02d}.npy'\n",
    "        times_fp = f'{constants.domains_dir}{domain}/times_{current_month.year}_{current_month.month:02d}.npy'\n",
    "        input_file_paths.append(input_fp)\n",
    "        target_file_paths.append(target_fp)\n",
    "        times_file_paths.append(times_fp)\n",
    "        current_month += relativedelta(months=1)\n",
    "\n",
    "    input_arr = np.concatenate([np.load(fp) for fp in input_file_paths])\n",
    "    target_arr = np.concatenate([np.load(fp) for fp in target_file_paths])\n",
    "    times_arr = np.concatenate([np.load(fp) for fp in times_file_paths])\n",
    "\n",
    "    times_arr = times_arr.astype('datetime64[s]').astype(np.float64)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    indices = np.argsort(times_arr)\n",
    "    np.random.shuffle(indices)\n",
    "    input_arr = input_arr[indices]\n",
    "    target_arr = target_arr[indices]\n",
    "    times_arr = times_arr[indices]\n",
    "\n",
    "    # convert the first 3 timestamps in times_arr to numpy datetime64, print them\n",
    "    print(times_arr[:3].astype('datetime64[s]'))\n",
    "\n",
    "    test_input_arr, train_input_arr = np.split(input_arr, [int(train_test * len(input_arr))])\n",
    "    test_target_arr, train_target_arr = np.split(target_arr, [int(train_test * len(target_arr))])\n",
    "    test_times_arr, train_times_arr = np.split(times_arr, [int(train_test * len(times_arr))])\n",
    "\n",
    "    train_dataset = MemMapDataset(train_input_arr, train_target_arr, train_times_arr)\n",
    "    test_dataset = MemMapDataset(test_input_arr, test_target_arr, test_times_arr)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(42)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=constants.training_batch_size, shuffle=True, generator=generator)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=constants.training_batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def find_adjascent_tiles(domain):\n",
    "    with open(f'{constants.domains_dir}grid_domains.pkl', 'rb') as f:\n",
    "        grid_domains = pickle.load(f)\n",
    "    min_lat, max_lat, min_lon, max_lon = grid_domains[domain]\n",
    "\n",
    "    adjascent_tiles = []\n",
    "\n",
    "    # left tile\n",
    "    coordinates = [min_lat, max_lat, min_lon-4, max_lon-4]\n",
    "    for domain in grid_domains:\n",
    "        if grid_domains[domain] == coordinates:\n",
    "            adjascent_tiles.append((domain, 'left'))\n",
    "\n",
    "    # right tile\n",
    "    coordinates = [min_lat, max_lat, min_lon+4, max_lon+4]\n",
    "    for domain in grid_domains:\n",
    "        if grid_domains[domain] == coordinates:\n",
    "            adjascent_tiles.append((domain, 'right'))\n",
    "\n",
    "    # above tile\n",
    "    coordinates = [min_lat+4, max_lat+4, min_lon, max_lon]\n",
    "    for domain in grid_domains:\n",
    "        if grid_domains[domain] == coordinates:\n",
    "            adjascent_tiles.append((domain, 'above'))\n",
    "\n",
    "    # below tile\n",
    "    coordinates = [min_lat-4, max_lat-4, min_lon, max_lon]\n",
    "    for domain in grid_domains:\n",
    "        if grid_domains[domain] == coordinates:\n",
    "            adjascent_tiles.append((domain, 'below'))\n",
    "\n",
    "    return adjascent_tiles\n",
    "\n",
    "\n",
    "all_losses = {}\n",
    "\n",
    "\n",
    "domain1 = 29\n",
    "first_month = (1979, 10)\n",
    "last_month = (1980, 9)\n",
    "train_test = 0.2\n",
    "\n",
    "adjascent_tiles = find_adjascent_tiles(domain1)\n",
    "\n",
    "for adjascent_tile in adjascent_tiles:\n",
    "    domain2, one_to_two_relationship = adjascent_tile[0], adjascent_tile[1]\n",
    "    print(domain2, one_to_two_relationship)\n",
    "\n",
    "    model1 = UNetWithAttention(1, 1, output_shape=(64,64)).to(constants.device).eval()\n",
    "    model2 = UNetWithAttention(1, 1, output_shape=(64,64)).to(constants.device).eval()\n",
    "\n",
    "    checkpoint1 = torch.load(f'{constants.checkpoints_dir}{domain1}/0_model.pt')\n",
    "    model1.load_state_dict(checkpoint1['model_state_dict'])\n",
    "\n",
    "    checkpoint2 = torch.load(f'{constants.checkpoints_dir}{domain2}/0_model.pt')\n",
    "    model2.load_state_dict(checkpoint2['model_state_dict'])\n",
    "\n",
    "    print('weights loaded')\n",
    "\n",
    "    train1_dataloader, test1_dataloader = generate_dataloaders(domain1, first_month, last_month, train_test)\n",
    "    train2_dataloader, test2_dataloader = generate_dataloaders(domain2, first_month, last_month, train_test)\n",
    "\n",
    "    print('dataloaders generated')\n",
    "\n",
    "    target_edge_losses = []\n",
    "    output_edge_losses = []\n",
    "\n",
    "    for i, ((inputs1, targets1, times1), (inputs2, targets2, times2)) in tqdm(enumerate(zip(test1_dataloader, test2_dataloader)), total=len(test1_dataloader)):\n",
    "\n",
    "        inputs1, targets1 = inputs1.to(constants.device), targets1.to(constants.device)\n",
    "        inputs2, targets2 = inputs2.to(constants.device), targets2.to(constants.device)\n",
    "\n",
    "        outputs1 = model1(inputs1)\n",
    "        outputs2 = model2(inputs2)\n",
    "\n",
    "        if one_to_two_relationship == 'left':\n",
    "            combined_target = np.concatenate([targets2.detach().cpu().numpy(), targets1.detach().cpu().numpy()], axis=2)\n",
    "            combined_output = np.concatenate([outputs2.detach().cpu().numpy(), outputs1.detach().cpu().numpy()], axis=2)\n",
    "\n",
    "            domain1_target_edge = targets1[:, :, 0]\n",
    "            domain2_target_edge = targets2[:, :, -1]\n",
    "            domain1_output_edge = outputs1[:, :, 0]\n",
    "            domain2_output_edge = outputs2[:, :, -1]\n",
    "        elif one_to_two_relationship == 'right':\n",
    "            combined_target = np.concatenate([targets1.detach().cpu().numpy(), targets2.detach().cpu().numpy()], axis=2)\n",
    "            combined_output = np.concatenate([outputs1.detach().cpu().numpy(), outputs2.detach().cpu().numpy()], axis=2)\n",
    "\n",
    "            domain1_target_edge = targets1[:, :, -1]\n",
    "            domain2_target_edge = targets2[:, :, 0]\n",
    "            domain1_output_edge = outputs1[:, :, -1]\n",
    "            domain2_output_edge = outputs2[:, :, 0]\n",
    "        elif one_to_two_relationship == 'above':\n",
    "            combined_target = np.concatenate([targets1.detach().cpu().numpy(), targets2.detach().cpu().numpy()], axis=1)\n",
    "            combined_output = np.concatenate([outputs1.detach().cpu().numpy(), outputs2.detach().cpu().numpy()], axis=1)\n",
    "\n",
    "            domain1_target_edge = targets1[:, -1, :]\n",
    "            domain2_target_edge = targets2[:, 0, :]\n",
    "            domain1_output_edge = outputs1[:, -1, :]\n",
    "            domain2_output_edge = outputs2[:, 0, :]\n",
    "        elif one_to_two_relationship == 'below':\n",
    "            combined_target = np.concatenate([targets2.detach().cpu().numpy(), targets1.detach().cpu().numpy()], axis=1)\n",
    "            combined_output = np.concatenate([outputs2.detach().cpu().numpy(), outputs1.detach().cpu().numpy()], axis=1)\n",
    "\n",
    "            domain1_target_edge = targets1[:, 0, :]\n",
    "            domain2_target_edge = targets2[:, -1, :]\n",
    "            domain1_output_edge = outputs1[:, 0, :]\n",
    "            domain2_output_edge = outputs2[:, -1, :]\n",
    "\n",
    "        edge_loss = nn.MSELoss()\n",
    "        target_edge_loss = edge_loss(domain1_target_edge, domain2_target_edge).item()\n",
    "        output_edge_loss = edge_loss(domain1_output_edge, domain2_output_edge).item()\n",
    "\n",
    "        target_edge_losses.append(target_edge_loss)\n",
    "        output_edge_losses.append(output_edge_loss)\n",
    "\n",
    "    print(np.mean(target_edge_losses))\n",
    "    print(np.mean(output_edge_losses))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
